import os
os.environ["USE_TF"] = "0"

import pickle
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import string
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

import os
from dotenv import load_dotenv

load_dotenv()  # Loads .env file

groq_api_key = os.getenv("GROQ_API_KEY")
print(groq_api_key)

# Groq API imports
from groq import Groq
import json

class EnhancedRAGSystemWithGroq:
    def __init__(self, vector_store_path='app/vector_store.pkl', groq_api_key=None):
        """Enhanced RAG System for Bengali Text with Groq API"""
        
        # Load vector store
        self.vector_store = self.load_vector_store(vector_store_path)
        
        # Load embedding model
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # Initialize Groq client
        if groq_api_key:
            self.groq_client = Groq(api_key=groq_api_key)
        else:
            # Try to get from environment variable
            api_key = os.getenv('GROQ_API_KEY')
            if api_key:
                self.groq_client = Groq(api_key=api_key)
            else:
                print("‚ö†Ô∏è Groq API key not provided. Add your API key to use LLM features.")
                self.groq_client = None
        
        # Initialize TF-IDF vectorizer for keyword matching
        self.tfidf_vectorizer = None
        self.chunk_tfidf_matrix = None
        
        # Bengali stop words - expanded list
        self.bengali_stop_words = {
            '‡¶Ü‡¶∞', '‡¶è‡¶∞', '‡¶§‡¶æ‡¶∞', '‡¶∏‡ßá', '‡¶§‡¶ø‡¶®‡¶ø', '‡¶Ü‡¶Æ‡¶ø', '‡¶Ü‡¶Æ‡¶æ‡¶∞', '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞', 
            '‡¶Ü‡¶™‡¶®‡¶ø', '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞', '‡¶è‡¶á', '‡¶ì‡¶á', '‡¶∏‡ßá‡¶á', '‡¶ê', '‡¶Ø‡ßá', '‡¶Ø‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶ï‡ßá',
            '‡¶ï‡ßá', '‡¶ï‡¶æ‡¶ï‡ßá', '‡¶ï‡¶æ‡¶∞', '‡¶ï‡¶ø', '‡¶ï‡ßÄ', '‡¶ï‡ßã‡¶®', '‡¶ï‡ßã‡¶®‡ßã', '‡¶ï‡¶§', '‡¶ï‡¶ñ‡¶®', 
            '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º', '‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá', '‡¶ï‡ßá‡¶®', '‡¶ï‡ßá‡¶Æ‡¶®', '‡¶ï‡¶ø‡¶∏‡ßá‡¶∞', '‡¶ï‡¶æ‡¶õ‡ßá', '‡¶ï‡¶æ‡¶õ ‡¶•‡ßá‡¶ï‡ßá',
            '‡¶π‡¶Ø‡¶º', '‡¶π‡¶≤‡ßã', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶π‡¶á‡¶≤', '‡¶π‡¶á‡¶Ø‡¶º‡¶æ‡¶õ‡ßá', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤', '‡¶π‡¶Ø‡¶º‡ßá', '‡¶π‡¶Ø‡¶º‡¶§',
            '‡¶¨‡¶≤‡¶æ', '‡¶¨‡¶≤‡ßá', '‡¶¨‡¶≤‡ßá‡¶õ‡ßá', '‡¶¨‡¶≤‡ßá‡¶õ‡¶ø‡¶≤', '‡¶¨‡¶≤‡ßá‡¶®', '‡¶¨‡¶≤‡¶≤', '‡¶¨‡¶≤‡¶≤‡ßá‡¶®',
            '‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ', '‡¶â‡¶≤‡ßç‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§', '‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ', '‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá',
            '‡¶ï‡¶∞‡¶æ', '‡¶ï‡¶∞‡ßá', '‡¶ï‡¶∞‡ßá‡¶õ‡ßá', '‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤', '‡¶ï‡¶∞‡¶≤', '‡¶ï‡¶∞‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶§‡ßá',
            '‡¶è‡¶¨‡¶Ç', '‡¶ì', '‡¶Ü‡¶ì', '‡¶¨‡¶æ', '‡¶Ö‡¶•‡¶¨‡¶æ', '‡¶§‡¶¨‡ßá', '‡¶Ø‡¶¶‡¶ø', '‡¶§‡¶æ‡¶π‡¶≤‡ßá', 
            '‡¶è‡¶∞', '‡¶è‡¶ï', '‡¶è‡¶ï‡¶ü‡¶ø', '‡¶è‡¶ï‡¶ú‡¶®', '‡¶¶‡ßÅ‡¶á', '‡¶¶‡ßÅ‡¶á‡¶ú‡¶®', '‡¶§‡¶ø‡¶®', '‡¶ö‡¶æ‡¶∞',
            '‡¶Ü‡¶õ‡ßá', '‡¶Ü‡¶õ‡¶ø‡¶≤', '‡¶õ‡¶ø‡¶≤', '‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶•‡¶æ‡¶ï‡ßá', '‡¶•‡ßá‡¶ï‡ßá', '‡¶•‡¶æ‡¶ï‡¶§‡ßá',
            '‡¶ó‡ßá‡¶õ‡ßá', '‡¶ó‡ßá‡¶≤', '‡¶ó‡ßá‡¶≤‡ßá‡¶®', '‡¶è‡¶≤', '‡¶è‡¶≤‡ßá‡¶®', '‡¶Ü‡¶∏‡¶≤', '‡¶Ü‡¶∏‡¶≤‡ßá‡¶®',
            '‡¶Ø‡¶æ‡¶Ø‡¶º', '‡¶Ø‡ßá‡¶§‡ßá', '‡¶Ü‡¶∏‡¶§‡ßá', '‡¶™‡¶æ‡¶∞‡ßá', '‡¶™‡¶æ‡¶∞‡ßá‡¶®', '‡¶™‡¶æ‡¶∞‡¶ø', '‡¶™‡¶æ‡¶∞‡ßã',
            '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá', '‡¶®‡¶ø‡¶Ø‡¶º‡ßá', '‡¶∏‡¶æ‡¶•‡ßá', '‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá','‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º','‡¶Ü‡¶∏‡¶ø‡¶§‡ßá', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶≠‡¶ø‡¶§‡¶∞‡ßá',
            '‡¶â‡¶™‡¶∞', '‡¶®‡¶ø‡¶ö‡ßá', '‡¶™‡¶æ‡¶∂‡ßá', '‡¶ï‡¶æ‡¶õ‡ßá', '‡¶¶‡ßÇ‡¶∞‡ßá', '‡¶∏‡¶æ‡¶Æ‡¶®‡ßá', '‡¶™‡¶ø‡¶õ‡¶®‡ßá',
            '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶ï‡¶æ‡¶∞‡¶£‡ßá', '‡¶´‡¶≤‡ßá', '‡¶§‡¶æ‡¶á', '‡¶∏‡ßá‡¶ú‡¶®‡ßç‡¶Ø', '‡¶è‡¶ú‡¶®‡ßç‡¶Ø', '‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ',
            '‡¶è‡¶ï‡¶ü‡¶æ', '‡¶è‡¶ï‡¶ü‡ßÅ', '‡¶Ö‡¶®‡ßá‡¶ï', '‡¶ï‡¶ø‡¶õ‡ßÅ', '‡¶∏‡¶¨', '‡¶∏‡¶ï‡¶≤', '‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶¨‡ßá‡¶∂‡¶ø',
            '‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º','‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ','‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞','‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ','‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞','‡¶ï‡¶Æ', '‡¶¨‡¶°‡¶º', '‡¶õ‡ßã‡¶ü', '‡¶≠‡¶æ‡¶≤', '‡¶≠‡¶æ‡¶≤‡ßã', '‡¶Æ‡¶®‡ßç‡¶¶', '‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø', '‡¶∏‡ßå‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø'
        }
        
        # Bengali suffixes that should be considered for word matching
        self.bengali_suffixes = [
            '‡¶ï‡ßá', '‡¶ï‡ßá‡¶á', '‡¶ü‡¶ø', '‡¶ü‡¶ø‡¶ï‡ßá', '‡¶ü‡¶æ', '‡¶ü‡¶æ‡¶ï‡ßá', '‡¶ñ‡¶æ‡¶®‡¶ø', '‡¶ñ‡¶æ‡¶®‡¶æ', '‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶ó‡ßÅ‡¶≤‡ßã',
            '‡¶¶‡ßá‡¶∞', '‡¶∞‡¶æ', '‡ßá‡¶∞‡¶æ', '‡¶ó‡¶£', '‡¶¨‡ßÉ‡¶®‡ßç‡¶¶', '‡¶Æ‡¶æ‡¶≤‡¶æ', '‡¶™‡ßÅ‡¶û‡ßç‡¶ú', '‡¶®‡¶ø‡¶ö‡¶Ø‡¶º', '‡¶∏‡¶Æ‡ßÇ‡¶π',
            '‡¶è', '‡¶§‡ßá', '‡¶Ø‡¶º', '‡¶∞', '‡¶è‡¶∞', '‡ßá‡¶∞', '‡¶á‡¶§‡ßá', '‡ßá‡¶§‡ßá', '‡¶π‡¶§‡ßá', '‡¶•‡ßá‡¶ï‡ßá',
            '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá', '‡¶®‡¶ø‡¶Ø‡¶º‡ßá', '‡¶õ‡¶æ‡¶°‡¶º‡¶æ', '‡¶¨‡¶ø‡¶®‡¶æ', '‡¶¨‡ßç‡¶Ø‡¶§‡ßÄ‡¶§', '‡¶∏‡¶π', '‡¶∏‡¶π‡¶ø‡¶§', '‡¶Æ‡¶§',
            '‡¶ì', '‡¶á', '‡¶ì', '‡¶á', '‡¶ü‡ßã', '‡¶ü‡¶æ‡¶á', '‡¶ü‡¶æ‡¶ì', '‡¶ñ‡¶æ‡¶®‡¶ø‡¶á', '‡¶ñ‡¶æ‡¶®‡¶æ‡¶ì',
            '‡¶ó‡ßÅ‡¶≤‡¶ø‡¶á', '‡¶ó‡ßÅ‡¶≤‡ßã‡¶ì', '‡¶¶‡ßá‡¶∞‡¶á', '‡¶∞‡¶æ‡¶á', '‡¶¶‡ßá‡¶∞‡¶ì', '‡¶∞‡¶æ‡¶ì', '‡¶ó‡¶£‡¶á', '‡¶ó‡¶£‡¶ì'
        ]
        
        # Prepare chunks for processing
        self.prepare_chunks()
    
    def load_vector_store(self, path):
        """Load the vector store from pickle file"""
        try:
            with open(path, 'rb') as f:
                store = pickle.load(f)
            print(f"‚úÖ Vector store loaded successfully with {len(store['chunks'])} chunks")
            return store
        except Exception as e:
            print(f"‚ùå Error loading vector store: {e}")
            return None
    
    def clean_text(self, text):
        """Advanced text cleaning for Bengali"""
        if not text:
            return ""
        
        # Remove extra whitespace and normalize
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # Remove punctuation but keep Bengali punctuation
        bengali_punctuation = '‡•§,;!?()[]{}"\'-‚Äì‚Äî'
        text = re.sub(f'[{re.escape(string.punctuation + bengali_punctuation)}]', ' ', text)
        
        # Remove extra spaces again
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def get_word_stem(self, word):
        """Get the stem of a Bengali word by removing common suffixes"""
        original_word = word
        
        # Sort suffixes by length (longest first) to match longer suffixes first
        sorted_suffixes = sorted(self.bengali_suffixes, key=len, reverse=True)
        
        for suffix in sorted_suffixes:
            if word.endswith(suffix) and len(word) > len(suffix) + 1:  # Ensure stem is not too short
                stem = word[:-len(suffix)]
                # Only return stem if it's meaningful (at least 2 characters)
                if len(stem) >= 2:
                    return stem
        
        return original_word
    
    def extract_keywords(self, text, min_length=2):
        """Extract meaningful keywords from Bengali text with stemming"""
        if not text:
            return []
        
        # Clean text
        clean_text = self.clean_text(text.lower())
        words = clean_text.split()
        
        # Filter and stem keywords
        keywords = []
        for word in words:
            if (len(word) >= min_length and 
                word not in self.bengali_stop_words and
                not word.isdigit()):
                
                # Get stem of the word
                stem = self.get_word_stem(word)
                keywords.append({
                    'original': word,
                    'stem': stem
                })
        
        return keywords
    
    def prepare_chunks(self):
        """Prepare chunks with TF-IDF processing"""
        if not self.vector_store or not self.vector_store['chunks']:
            print("‚ùå No chunks found in vector store")
            return
        
        chunks = self.vector_store['chunks']
        
        # Clean and prepare chunks for TF-IDF
        cleaned_chunks = []
        for chunk in chunks:
            cleaned = self.clean_text(chunk)
            if cleaned:
                cleaned_chunks.append(cleaned)
            else:
                cleaned_chunks.append(chunk)  # fallback to original
        
        # Create TF-IDF matrix
        try:
            self.tfidf_vectorizer = TfidfVectorizer(
                lowercase=True,
                stop_words=None,  # We'll handle stop words manually
                ngram_range=(1, 2),  # Include bigrams
                max_features=5000,
                min_df=1,
                max_df=0.8
            )
            
            self.chunk_tfidf_matrix = self.tfidf_vectorizer.fit_transform(cleaned_chunks)
            print(f"‚úÖ TF-IDF matrix created: {self.chunk_tfidf_matrix.shape}")
            
        except Exception as e:
            print(f"‚ùå Error creating TF-IDF matrix: {e}")
            self.tfidf_vectorizer = None
            self.chunk_tfidf_matrix = None
    
    def improved_keyword_matching(self, query_keywords, chunk_text):
        """Improved keyword matching with stem-based comparison"""
        if not query_keywords:
            return 0, []
        
        chunk_keywords = self.extract_keywords(chunk_text)
        
        exact_matches = []
        stem_matches = []
        
        for q_keyword in query_keywords:
            q_original = q_keyword['original']
            q_stem = q_keyword['stem']
            
            # Check for exact matches first
            found_exact = False
            for c_keyword in chunk_keywords:
                c_original = c_keyword['original']
                c_stem = c_keyword['stem']
                
                # Exact original word match
                if q_original == c_original:
                    exact_matches.append(q_original)
                    found_exact = True
                    break
                
                # Check if query word appears as part of chunk word (but with suffixes)
                # This handles cases like "‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡¶¶‡ßá‡¶¨‡¶§‡¶æ" matching "‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡¶¶‡ßá‡¶¨‡¶§‡¶æ‡¶ï‡ßá"
                elif c_original.startswith(q_original) and len(c_original) > len(q_original):
                    # Check if the extra part is a valid suffix
                    extra_part = c_original[len(q_original):]
                    if extra_part in self.bengali_suffixes:
                        exact_matches.append(q_original)
                        found_exact = True
                        break
                
                # Check reverse: chunk word appears in query word with suffix
                elif q_original.startswith(c_original) and len(q_original) > len(c_original):
                    extra_part = q_original[len(c_original):]
                    if extra_part in self.bengali_suffixes:
                        exact_matches.append(q_original)
                        found_exact = True
                        break
            
            # If no exact match found, check stem matches
            if not found_exact:
                for c_keyword in chunk_keywords:
                    c_stem = c_keyword['stem']
                    
                    # Stem match (less weight)
                    if q_stem == c_stem and len(q_stem) >= 3:  # Only consider meaningful stems
                        stem_matches.append(q_original)
                        break
        
        # Calculate match count with different weights
        exact_count = len(exact_matches)
        stem_count = len(stem_matches) * 0.7  # Stem matches get less weight
        
        total_matches = exact_count + stem_count
        all_matches = exact_matches + stem_matches
        
        return total_matches, all_matches
    
    def keyword_similarity_score(self, query_keywords, chunk_text):
        """Calculate keyword-based similarity score with improved matching"""
        if not query_keywords:
            return 0.0
        
        match_count, matching_keywords = self.improved_keyword_matching(query_keywords, chunk_text)
        
        if len(query_keywords) == 0:
            return 0.0
        
        # Calculate similarity based on match ratio
        similarity = match_count / len(query_keywords)
        
        return min(1.0, similarity)
    
    def tfidf_similarity_score(self, query):
        """Calculate TF-IDF based similarity"""
        if not self.tfidf_vectorizer or self.chunk_tfidf_matrix is None:
            return []
        
        try:
            # Transform query using fitted vectorizer
            query_cleaned = self.clean_text(query)
            query_tfidf = self.tfidf_vectorizer.transform([query_cleaned])
            
            # Calculate cosine similarity
            similarities = cosine_similarity(query_tfidf, self.chunk_tfidf_matrix).flatten()
            
            return similarities
        except Exception as e:
            print(f"‚ùå Error in TF-IDF similarity: {e}")
            return []
    
    def semantic_similarity_score(self, query, chunk):
        """Calculate semantic similarity using sentence transformers"""
        try:
            query_emb = self.embedding_model.encode([query])
            chunk_emb = self.embedding_model.encode([chunk])
            
            similarity = cosine_similarity(query_emb, chunk_emb)[0][0]
            return float(similarity)
            
        except Exception as e:
            print(f"‚ùå Error in semantic similarity: {e}")
            return 0.0
    
    def find_best_chunks(self, query, top_k=10, debug=True):
        """Find the most relevant chunks with improved keyword matching"""
        
        if not self.vector_store or not self.vector_store['chunks']:
            print("‚ùå No chunks available")
            return []
        
        chunks = self.vector_store['chunks']
        query_keywords = self.extract_keywords(query)
        
        if debug:
            print(f"\nüîç Query: {query}")
            print(f"üìù Extracted Keywords: {[kw['original'] for kw in query_keywords]}")
            print(f"üìä Total Chunks to Search: {len(chunks)}")
            print("-" * 80)
        
        # Get TF-IDF similarities for all chunks
        tfidf_similarities = self.tfidf_similarity_score(query)
        
        scored_chunks = []
        
        for i, chunk in enumerate(chunks):
            # 1. Improved keyword matching (PRIMARY CRITERIA)
            match_count, matching_kw = self.improved_keyword_matching(query_keywords, chunk)
            
            # 2. Keyword similarity score
            keyword_score = self.keyword_similarity_score(query_keywords, chunk)
            
            # 3. TF-IDF similarity
            tfidf_score = tfidf_similarities[i] if i < len(tfidf_similarities) else 0.0
            
            # 4. Semantic similarity (only for candidates with keyword matches)
            semantic_score = 0.0
            if match_count > 0:
                semantic_score = self.semantic_similarity_score(query, chunk)
            
            # 5. Length normalization
            chunk_words = len(chunk.split())
            length_score = min(1.0, chunk_words / 50) if chunk_words > 0 else 0.0
            
            # Combined weighted score with heavy emphasis on keyword matching
            combined_score = (
                0.60 * match_count / len(query_keywords) if query_keywords else 0 +  # Match count ratio (highest weight)
                0.20 * keyword_score +          # Keyword similarity
                0.10 * tfidf_score +           # TF-IDF similarity  
                0.05 * semantic_score +        # Semantic similarity
                0.05 * length_score            # Length normalization
            )
            
            # Only include chunks with keyword matches
            if match_count > 0:
                scored_chunks.append({
                    'text': chunk,
                    'index': i,
                    'combined_score': combined_score,
                    'match_count': match_count,
                    'keyword_score': keyword_score,
                    'tfidf_score': tfidf_score,
                    'semantic_score': semantic_score,
                    'length_score': length_score,
                    'matching_keywords': matching_kw
                })
        
        # Sort by match count first, then by combined score
        scored_chunks.sort(key=lambda x: (x['match_count'], x['combined_score']), reverse=True)
        
        if debug and scored_chunks:
            print(f"\nüéØ Top {min(5, len(scored_chunks))} Results (Sorted by Keyword Matches):")
            print("=" * 120)
            
            for rank, chunk_info in enumerate(scored_chunks[:5], 1):
                print(f"\n[Rank {rank}] üî• Keyword Matches: {chunk_info['match_count']:.1f} | Combined Score: {chunk_info['combined_score']:.3f}")
                print(f"   üìä Keyword: {chunk_info['keyword_score']:.3f} | "
                      f"TF-IDF: {chunk_info['tfidf_score']:.3f} | "
                      f"Semantic: {chunk_info['semantic_score']:.3f}")
                print(f"   üéØ Matching Keywords: {chunk_info['matching_keywords']}")
                print(f"   üìÑ Text Preview: {chunk_info['text'][:200]}...")
                print("-" * 80)
        
        return scored_chunks[:top_k]
    
    def generate_answer_with_groq(self, query, chunk_text):
        """Generate concise answer using Groq API"""
        if not self.groq_client:
            print("‚ö†Ô∏è Groq API not available. Using fallback method.")
            return self.extract_answer_fallback(query, chunk_text)
        
        try:
            # Create prompt for Groq
            prompt = f"""‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û‡•§ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßã‡•§

‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {query}

‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó: {chunk_text}

‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:
- ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì
- ‡¶è‡¶ï ‡¶ï‡¶•‡¶æ‡¶Ø‡¶º ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì
- ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶æ‡¶Æ ‡¶π‡¶Ø‡¶º, ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶®‡¶æ‡¶Æ‡¶ü‡¶ø ‡¶¶‡¶æ‡¶ì
- ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶ü‡¶ø ‡¶¶‡¶æ‡¶ì
- ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶π‡¶Ø‡¶º, ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ‡¶ü‡¶ø ‡¶¶‡¶æ‡¶ì
- ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¶‡¶ø‡¶ì ‡¶®‡¶æ
- ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶®‡¶æ ‡¶™‡¶æ‡¶ì, "‡¶§‡¶•‡ßç‡¶Ø ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø" ‡¶¨‡¶≤‡ßã

‡¶â‡¶§‡ßç‡¶§‡¶∞:"""

            # Try different models in order of preference
            models_to_try = [
                "llama-3.3-70b-versatile",
                "llama-3.1-8b-instant", 
                "mixtral-8x7b-32768",
                "gemma2-9b-it"
            ]
            
            response = None
            for model in models_to_try:
                try:
                    response = self.groq_client.chat.completions.create(
                        messages=[
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        model=model,
                        temperature=0.1,  # Low temperature for more focused answers
                        max_tokens=50,  # Short answers
                        top_p=0.9
                    )
                    print(f"‚úÖ Using model: {model}")
                    break
                except Exception as model_error:
                    print(f"‚ùå Model {model} failed: {model_error}")
                    continue
            
            if response is None:
                raise Exception("All models failed")
            
            answer = response.choices[0].message.content.strip()
            
            # Clean up the answer
            answer = re.sub(r'^(‡¶â‡¶§‡ßç‡¶§‡¶∞:|Answer:|A:)', '', answer).strip()
            answer = answer.strip('‡•§')  # Remove ending punctuation if any
            
            return answer
            
        except Exception as e:
            print(f"‚ùå Error with Groq API: {e}")
            return self.extract_answer_fallback(query, chunk_text)
   
    
    def extract_answer_fallback(self, query, chunk_text):
        """Fallback method for answer extraction without API"""
        # Split into sentences
        sentences = re.split(r'[‡•§!?]', chunk_text)
        query_keywords = self.extract_keywords(query)
        
        if not query_keywords or not sentences:
            return chunk_text.strip()[:100]  # Return first 100 chars
        
        # Score each sentence
        sentence_scores = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:
                # Calculate keyword overlap for this sentence
                match_count, _ = self.improved_keyword_matching(query_keywords, sentence)
                
                # Calculate score based on keyword matches
                if match_count > 0:
                    sentence_scores.append((sentence, match_count))
        
        if sentence_scores:
            # Sort by match count
            sentence_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Return best sentence if it has good overlap
            if sentence_scores[0][1] >= 1:  # At least 1 keyword match
                return sentence_scores[0][0]
        
        # Fallback to first 100 characters
        return chunk_text.strip()[:100]
    
    def search_and_answer(self, query, top_k=5, use_groq=True):
        """Main function to search and provide answer"""
        
        best_chunks = self.find_best_chunks(query, top_k=top_k, debug=True)
        
        if not best_chunks:
            return {
                'answer': "‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø‡•§",
                'confidence': 0.0,
                'keyword_matches': 0,  # Added missing key
                'source_chunks': [],
                'full_chunk': "",
                'matching_keywords': []  # Added missing key
            }
        
        # Get the best chunk (highest keyword match count)
        best_chunk = best_chunks[0]
        
        # Get full chunk text (this was the issue - it was being truncated)
        full_chunk_text = best_chunk['text']
        
        # Generate short answer using Groq API
        if use_groq and self.groq_client:
            short_answer = self.generate_answer_with_groq(query, full_chunk_text)
        else:
            short_answer = self.extract_answer_fallback(query, full_chunk_text)
        
        return {
            'answer': short_answer,  # Short answer from Groq
            'full_chunk': full_chunk_text,  # Complete chunk text
            'confidence': best_chunk['combined_score'],
            'keyword_matches': best_chunk['match_count'],
            'source_chunks': best_chunks[:3],  # Top 3 chunks for reference
            'matching_keywords': best_chunk['matching_keywords']
        } 
    
    def interactive_mode(self):
        """Interactive question-answering mode"""
        print("\n" + "="*60)
        print("ü§ñ Enhanced Bengali RAG System with Groq Integration")
        print("="*60)
        print("üìù Type your questions in Bengali")
        print("‚èπÔ∏è  Type 'exit', 'quit', or 'bye' to stop")
        print("="*60)
        
        while True:
            try:
                user_query = input("\n‚ùì ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®: ").strip()
                
                if user_query.lower() in ['exit', 'quit', 'bye', '‡¶¨‡¶æ‡¶á', '‡¶¨‡¶®‡ßç‡¶ß']:
                    print("üëã ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶! RAG System ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...")
                    break
                
                if not user_query:
                    print("‚ö†Ô∏è ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®!")
                    continue
                
                # Get answer
                result = self.search_and_answer(user_query, use_groq=True)
                
                print(f"\nüéØ ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶â‡¶§‡ßç‡¶§‡¶∞: {result['answer']}")
                print(f"üìä ‡¶ï‡¶®‡¶´‡¶ø‡¶°‡ßá‡¶®‡ßç‡¶∏ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞: {result['confidence']:.3f}")
                print(f"üî¢ ‡¶ï‡ßÄ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö: {result['keyword_matches']:.1f}")
                
                if result['matching_keywords']:
                    print(f"üîç ‡¶Æ‡¶ø‡¶≤‡ßá‡¶õ‡ßá ‡¶è‡¶á ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã: {', '.join(result['matching_keywords'])}")
                
                # Show full chunk if requested
                show_full = input("\nüìÑ ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ö‡¶æ‡¶®? (y/n): ").lower()
                if show_full in ['y', 'yes', '‡¶π‡ßç‡¶Ø‡¶æ‡¶Å', '‡¶π‡¶æ']:
                    print(f"\nüìÑ ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó:\n{result['full_chunk']}")
                
                print("\n" + "-"*60)
                
            except KeyboardInterrupt:
                print("\n\nüëã RAG System ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")


def test_rag_system_with_groq():
    """Test the RAG system with Groq API"""
    
    # Initialize RAG system with Groq
    rag = EnhancedRAGSystemWithGroq(groq_api_key=None)  # Will use GROQ_API_KEY env var
    
    # Test queries
    test_queries = [
        "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
        "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
        "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶™‡¶≤‡¶ï‡ßç‡¶∑‡ßá ‡¶ï‡¶®‡ßç‡¶Ø‡¶æ‡¶™‡¶ï‡ßç‡¶∑‡¶ï‡ßá ‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ü‡¶∏‡¶ø‡¶§‡ßá ‡¶π‡¶á‡¶≤?"
    ]
    
    print("\n" + "="*80)
    print("üìã RAG System Testing with Groq Integration")
    print("="*80)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n[Test {i}] üîç Query: {query}")
        print("-" * 80)
        
        result = rag.search_and_answer(query, top_k=3, use_groq=True)
        
        print(f"‚úÖ Short Answer: {result['answer']}")
        print(f"üìä Confidence: {result['confidence']:.3f}")
        print(f"üî¢ Keyword Matches: {result.get('keyword_matches', 0):.1f}")
        
        if result.get('matching_keywords'):
            print(f"üéØ Keywords Found: {', '.join(result['matching_keywords'])}")
        
        if result.get('full_chunk'):
            print(f"\nüìÑ Full Chunk Preview: {result['full_chunk'][:300]}...")
        print("=" * 80)


if __name__ == "__main__":
    # Test the system first
    print("üöÄ Testing Enhanced RAG System with Groq Integration...")
    test_rag_system_with_groq()
    
    # Start interactive mode
    print("\nüîÑ Starting Interactive Mode...")
    rag = EnhancedRAGSystemWithGroq()
    rag.interactive_mode()